<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AI Pocket Reference: NLP</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="A streamlined reference manual for AI practitioners, students, and developers to quickly look up core concepts and mock implementations">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">AI Pocket Reference: NLP</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="lora"><a class="header" href="#lora">LoRA</a></h1>
<!-- markdownlint-disable MD013 -->
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <a target="_blank" href="https://colab.research.google.com/github/VectorInstitute/ai-pocket-reference-code/blob/main/notebooks/nlp/lora.ipynb">
      <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 3 minutes</small></p>
  </div>
</div>
<!-- markdownlint-enable MD013 -->
<p>Low-rank adaptation (LoRA) is parameter-efficient fine-tuning (<a href="llms/fine_tuning/../fine_tuning/peft.html">PEFT</a>)
introduced by Hu, Edward J. et al. (2021). The creators of LoRA posited that since
trained deep learning models reside in low intrinsic dimensions, perhaps their
weight-update matrices do as well.</p>
<p>Specifically, with LoRA, we learn a low-rank representation of the weight-update
matrices of dense, linear layers of a pre-trained LLM. The original weights
of the LLM are frozen during fine-tuning and only the low-rank weight-update matrices
at each step of fine-tuning. This reduction in dimensionality helps to amplify the
most important or influential features of the model.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/LoRA.svg" alt="lora"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Illustrating a forward pass with LoRA
</div>
<h2 id="some-math"><a class="header" href="#some-math">Some Math</a></h2>
<p>Let \(W\) represent the \(d\times d\) weight matrix for a dense, linear layer.
We can then loosely represent an updated version (i.e. after fine-tuning) of
this matrix as follows:</p>
<p>$$W_{\text{updated}} = W + \Delta W,$$</p>
<p>where \(\Delta W\) is the update matrix. With LoRA, it is \(\Delta W\) which
we project into a low-rank space:</p>
<p>$$\Delta W \approx AB,$$</p>
<p>where \(A\) and \(B^T\) are both matrices of dimension \(d \times r\) and
\(r &lt;&lt; d\). During fine-tuning, \(W\) is frozen and only \(A\) and \(B\)
are updated.</p>
<p>For inference (i.e., forward phase), let \(x\) be an input embedding, then by
the distributive property</p>
<p>$$xW_{\text{updated}} = xW + x\Delta W \approx xW + xAB.$$</p>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<p>One modular implementation of LoRA involves the introduction of a <code>LoRALayer</code> that
comprises of only the \(A\) and \(B\) dense weights. In this way, a <code>LoRALayer</code>
can adapt a pre-trained <code>Linear</code> layer.</p>
<pre><code class="language-python">import torch


class LoRALayer(torch.nn.Module):
    """A basic LoRALayer implementation."""

    def __init__(self, d_in: int, d_out: int, rank: int):
        self.A = torch.nn.Parameter(torch.empty(d_in, rank))
        self.B = torch.nn.Parameter(torch.zeros(rank, d_out))

    def forward(self, x):
        return x @ self.A @ self.B
</code></pre>
<p>With the <code>LoRALayer</code> defined in this way, one can then combine this with a <code>Linear</code>
layer to implement the LoRA technique. See the supplementary Colab notebook linked
at the top of this pocket reference for more details.</p>
<h2 id="performance"><a class="header" href="#performance">Performance</a></h2>
<p>In the original paper, the authors reported similar levels of performance when using
LoRA compared to full fine-tuning on various natural language generation and understanding
tasks.</p>
<h2 id="additional-benefits"><a class="header" href="#additional-benefits">Additional Benefits</a></h2>
<p>Since LoRA matrices can be stored efficiently and separately from the pre-trained
LLM weights, customization of these large models is highly scalable. Organizations
can build libraries of specialized LoRA matrices for different datasets and domains,
switching between them as needed for specific applications.</p>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<h4 id="references--useful-links"><a class="header" href="#references--useful-links">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2106.09685"><em>Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models."
arXiv preprint arXiv:2106.09685, 2021.</em></a></li>
<li><a href="https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167"><em>Raschka, Sebastian. Build a Large Language Model (From Scratch). Simon and
Schuster, 2024.</em></a></li>
<li><a href="https://huggingface.co/docs/peft/en/task_guides/lora_based_methods"><em>Sourab Mangrulkar et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods (LoRA methods), 2022.</em></a></li>
<li><a href="https://arxiv.org/pdf/2307.13269"><em>Huang, Chengsong, et al. "Lorahub: Efficient cross-task generalization via
dynamic lora composition." arXiv preprint arXiv:2307.13269 (2023).</em></a></li>
<li><a href="https://d3ddy8balm3goa.cloudfront.net/paper-cards/w29_2023-lora.excalidraw.svg"><em>Fajardo V.A. LoRA PaperCard, 2023.</em></a></li>
</ol>
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="deepseek-r1"><a class="header" href="#deepseek-r1">DeepSeek-R1</a></h1>
<!-- markdownlint-disable MD013 -->
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
  </div>
</div>
<!-- markdownlint-enable MD013 -->
<p>The DeepSeek-R1 model was introduced by DeepSeek in January of 2025. It is
derived from an earlier checkpoint of <a href="models/../models/deepseek_v3.html">DeepSeek-V3</a>.
In particular, starting with DeepSeek-V3-base, four stages of fine-tuning were
performed in order to arrive at the checkpoint known as DeepSeek-R1: (i) <strong>Reasoning
Cold-Start</strong> (using <a href="models/../llms/fine_tuning/sft.html">SFT</a>), (ii) <strong>RL for Reasoning</strong>
(using <a href="models/../llms/fine_tuning/grpo.html">GRPO</a>), (iii) <strong>SFT for Enhanced Reasoning
&amp; General Capabilities</strong> (using RL-generated reasoning data sampled with
<a href="models/../llms/misc/rejection_sampling.html">Rejection Sampling</a>), and (iv) <strong>RL for Alignment</strong>
(to human preferences).</p>
<p><img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/deepseek-v3-r1-lineage.excalidraw.svg" alt="Lineage" /></p>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
<p>Figure: Illustrating DeepSeek-R1 model evolution.</p>
</div>
<p>As illustrated in the Figure above, the model lineage of DeepSeek-R1 implements
a full-scale RL for reasoning stage that leverages cold-start data. In contrast,
DeepSeek-R1-Zero does not use any cold-start SFT data whatsoever and uses purely
RL steps to acquire its reasoning capabilities. The reward signal used for
guiding the RL process of DeepSeek-R1-Zero is rules based computed from the
response's correctness as well as its adherence to the desired format. While
DeepSeek-R1-Zero demonstrated remarkable reasoning capabilities, it suffered greatly
from poor readability and language mixing.</p>
<p>This motivated the usage of cold-start data in the RL for Reasoning stage of
DeepSeek-R1's training. Additionally, a reward signal to reduce language mixing
as well as a model-based reward (using DeepSeek-V3 for judgement) was also
incorporated.</p>
<h2 id="historical-significance"><a class="header" href="#historical-significance">Historical Significance</a></h2>
<p>At the time of its release, LLM reasoning models such as the OpenAI's o-series
models had demonstrated remarkable performance on complex tasks, including those
requiring multiple steps (e.g., <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">OpenAI o3's breakthrough score on ARC-AGI</a>).
However, OpenAI—operating under a closed-source model—provided little details to
how these models were developed, merely mentioning that Reinforcement Learning techniques
were used to train the LLMs to produce long (internal) chain-of-thought style
reasoning prior to providing a final answer.</p>
<p>In contrast, DeepSeek open-sourced DeepSeek-R1 and provided a very detailed
technical report, shedding much light on its training pipeline, which included an
RL approach for the model to acquire its reasoning capabilities. It was also
reported that DeepSeek-R1 was trained on NVIDIA H800's, a less capable GPU than
the NVIDIA H100 or A100.</p>
<blockquote>
<p>DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs.</p>
<p><em>(quoted from the DeepSeek-V3 Technical Report)</em></p>
</blockquote>
<p>The fact that DeepSeek-R1's performance rivaled that of it's closed-source
counterpart in OpenAI o3 on multiple benchmarks (using reportedly less compute)
led to a frenzy in the LLM and broader AI community. As an example, many teams
(including at least one from HuggingFace) worked tirelessly to produce their own
versions of DeepSeek-R1 in the days after its release.</p>
<h2 id="architectural-highlights"><a class="header" href="#architectural-highlights">Architectural Highlights</a></h2>
<p>See <a href="models/../models/deepseek_v3.html">DeepSeek-V3</a>.</p>
<h2 id="training-data"><a class="header" href="#training-data">Training Data</a></h2>
<p>The training data used for the four stages are described below:</p>
<p><strong>Reasoning Cold Start</strong>: 1000s of samples of long CoT passages from multiple domains,
verified by human annotators was used.</p>
<p><strong>RL for Reasoning</strong>: self-exploration, using increased test-time for RL discovery
until convergence (referred to as the RL checkpoint).</p>
<p><strong>SFT for Enhanced Reasoning &amp; General Capabilities</strong>: the RL checkpoint was then
used to generate 600K reasoning related samples (using rejection sampling).
DeepSeek-V3 was used to create 200K non-reasoning data omitting the CoT portion
for simple queries.</p>
<p><strong>RL for Alignment</strong>: a combination of reward signals diverse data distributions
including preference pairs and analyses of generated summaries &amp; responses.</p>
<h2 id="key-results"><a class="header" href="#key-results">Key Results</a></h2>
<p>Below are three key results of DeepSeek-R1 and its development:</p>
<!-- markdownlint-disable MD013 -->
<div class="table-wrapper"><table><thead><tr><th>Benchmark (Metric)</th><th>Claude-3.5-Sonnet-1022</th><th>GPT-4 0513</th><th>DeepSeek-V3</th><th>OpenAI o1-mini</th><th>OpenAI o1-1217</th><th>DeepSeek-R1</th></tr></thead><tbody>
<tr><td>MMLU (Pass@1)</td><td>88.3</td><td>87.2</td><td>88.5</td><td>85.2</td><td><strong>91.8</strong></td><td>90.8</td></tr>
<tr><td>MMLU-Redux (EM)</td><td>88.9</td><td>88.0</td><td>89.1</td><td>86.7</td><td>-</td><td><strong>92.9</strong></td></tr>
<tr><td>MMLU-Pro (EM)</td><td>78.0</td><td>72.6</td><td>75.9</td><td>80.3</td><td>-</td><td><strong>84.0</strong></td></tr>
<tr><td>DROP (3-shot F1)</td><td>88.3</td><td>83.7</td><td>91.6</td><td>83.9</td><td>90.2</td><td><strong>92.2</strong></td></tr>
<tr><td>IF-Eval (Prompt Strict)</td><td><strong>86.5</strong></td><td>84.3</td><td>86.1</td><td>84.8</td><td>-</td><td>83.3</td></tr>
<tr><td>GFQA Diamond (Pass@1)</td><td>65.0</td><td>49.9</td><td>59.1</td><td>60.0</td><td><strong>75.7</strong></td><td>71.5</td></tr>
<tr><td>SimpleQA (Correct)</td><td>28.4</td><td>38.2</td><td>24.9</td><td>7.0</td><td><strong>47.0</strong></td><td>30.1</td></tr>
<tr><td>FRAMES (Acc.)</td><td>72.5</td><td>80.5</td><td>73.3</td><td>76.9</td><td>-</td><td><strong>82.5</strong></td></tr>
<tr><td>AlpacaEval2.0 (LC-winrate)</td><td>52.0</td><td>51.1</td><td>70.0</td><td>57.8</td><td>-</td><td><strong>87.6</strong></td></tr>
<tr><td>ArenaHard (GPT-4-1106)</td><td>85.2</td><td>80.4</td><td>85.5</td><td>92.0</td><td>-</td><td><strong>92.3</strong></td></tr>
<tr><td>LiveCodeBench (Pass@1-COT)</td><td>38.9</td><td>32.9</td><td>36.2</td><td>53.8</td><td>63.4</td><td><strong>65.9</strong></td></tr>
<tr><td>Codeforces (Percentile)</td><td>20.3</td><td>23.6</td><td>58.7</td><td>93.4</td><td><strong>96.6</strong></td><td>96.3</td></tr>
<tr><td>Codeforces (Rating)</td><td>717</td><td>759</td><td>1134</td><td>1820</td><td><strong>2061</strong></td><td>2029</td></tr>
<tr><td>SWE Verified (Resolved)</td><td><strong>50.8</strong></td><td>38.8</td><td>42.0</td><td>41.6</td><td>48.9</td><td>49.2</td></tr>
<tr><td>Aider-Polyglot (Acc.)</td><td>45.3</td><td>16.0</td><td>49.6</td><td>32.9</td><td><strong>61.7</strong></td><td>53.3</td></tr>
<tr><td>AIME 2024 (Pass@1)</td><td>16.0</td><td>9.3</td><td>39.2</td><td>63.6</td><td>79.2</td><td><strong>79.8</strong></td></tr>
<tr><td>MATH-500 (Pass@1)</td><td>78.3</td><td>74.6</td><td>90.2</td><td>90.0</td><td>96.4</td><td><strong>97.3</strong></td></tr>
<tr><td>CNMO 2024 (Pass@1)</td><td>13.1</td><td>10.8</td><td>43.2</td><td>67.6</td><td>-</td><td><strong>78.8</strong></td></tr>
<tr><td>CLUEWSC (EM)</td><td>85.4</td><td>87.9</td><td>90.9</td><td>89.9</td><td>-</td><td><strong>92.8</strong></td></tr>
<tr><td>C-Eval (EM)</td><td>76.7</td><td>76.0</td><td>86.5</td><td>68.9</td><td>-</td><td><strong>91.8</strong></td></tr>
<tr><td>C-SimpleQA (Correct)</td><td>55.4</td><td>58.7</td><td><strong>68.0</strong></td><td>40.3</td><td>-</td><td>63.7</td></tr>
</tbody></table>
</div><!-- markdownlint-enable MD013 -->
<div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
<p>Table: Comparison between DeepSeek-R1 and other representative models.
(Copied from Table 4 of Guo, Daya, et al (2025).)</p>
</div>
<ol>
<li>
<p><strong>Performance on Benchmarks:</strong> The table above which was copied from the DeepSeek-R1
paper compares the performance of DeepSeek-R1 and -V3 with representative models
from Anthropic and OpenAI. The values reported clearly demonstrate the impressive
performance of DeepSeek-R1 across various benchmarks and tasks. Most notably,
DeepSeek-R1 was able to surpass OpenAI's reasoning model o1-1217 on several benchmarks.</p>
</li>
<li>
<p><strong>Distilling Reasoning Capabilities:</strong> The 800K samples that included generated
examples by both DeepSeek-R1 (reasoning) and DeepSeek-V3 (non-reasoning) were
used to distill other open-source models like <a href="models/../models/qwen2pt5.html">Qwen</a>
and <a href="models/../models/llama_3.html">Llama</a>. With only the application SFT (i.e., no RL),
some of these distilled models were not only able to outperform OpenAI's non-reasoning
model GPT-4o-0513 across all benchmarks tested, but also OpenAI's o1-mini model
on most benchmarks.</p>
</li>
<li>
<p><strong>RL's Potential:</strong> Pure RL empowered DeepSeek-R1-Zero to autonomously acquire
robust reasoning capabilities without any SFT data. What's more is that as test-time
computation was increased, desirable behaviours such as reflection and re-evaluation
on past trajectories emerged making it possible for the model to have "aha moments"
when solving complex tasks. This development should serve as a reminder of the
great potential of RL and its overall place in AI as endeavour to reach new
heights.</p>
</li>
</ol>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<p>DeepSeek reported various limitations for DeepSeek-R1. Most notably, DeepSeek-R1
is inferior to DeepSeek-V3 in general capabilities such as function calling, producing
structured outputs (JSON), role-playing, and multi-turn conversations. Additionally,
due to its optimization for English and Chinese, the model sometimes suffers from
language mixing. Lastly, DeepSeek-R1 reportedly demonstrated a high sensitivity
to prompts and long inference times, making it unsuitable for low-latency applications
such as software-engineering tasks.</p>
<h4 id="references--useful-links-1"><a class="header" href="#references--useful-links-1">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2501.12948"><em>Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).</em></a></li>
<li><a href="https://arxiv.org/pdf/2412.19437"><em>Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint
arXiv:2412.19437 (2024).</em></a></li>
<li><a href="https://fortune.com/2025/01/27/china-deepseek-nvidia-gpu-investor-panic-us-export-controls-rethink/"><em>China's DeepSeek sets off Nvidia investor panic over US export controls</em></a>
<em>(appearing in fortune.com)</em></li>
<li><a href="https://huggingface.co/blog/open-r1"><em>Open-R1: a fully open reproduction of DeepSeek-R1</em></a>
<em>(by HuggingFace)</em></li>
<li><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"><em>DeepSeek-R1 is available on HuggingFace</em></a></li>
</ol>
<!-- markdownlint-disable-file MD033 -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="deepseek-v3"><a class="header" href="#deepseek-v3">DeepSeek-v3</a></h1>
<!-- markdownlint-disable MD013 -->
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 7 minutes</small></p>
  </div>
</div>
<!-- markdownlint-enable MD013 -->
<p>The DeepSeek-V3 model was introduced by DeepSeek in December of 2024. It is an
LLM that leverages <a href="models/../llms/architecture/moe.html">MoE</a> in its design.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/deepseek-v3-lineage-v2.excalidraw.svg" alt="DeepSeek-V3 Model Lineage"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Illustrating DeepSeek-V3 training evolution.
</div>
<p>The training pipeline for DeepSeek-V3 consists of the two typical stages: pre-training
and post-training. As depicted in the Figure above, the pre-training stage involves
pre-training on 14.8T tokens followed by long-context extension using the <a href="models/../llms/fine_tuning/yarn.html">YaRN</a>
methodology. Post-training of DeepSeek-V3 utilizes <a href="models/../llms/fine_tuning/sft.html">SFT</a>
as well as Reinforcement Learning methods.</p>
<h2 id="historical-significance-1"><a class="header" href="#historical-significance-1">Historical Significance</a></h2>
<p>At the time of its release, open-source models had already been lessening the gap
in performance with closed-source counterparts. DeepSeek-V3 was yet another open-source
model that achieved high levels of performance, beating other open-source alternatives
as well as some closed-source models in various benchmarks. What made DeepSeek-V3's
achievement even more intriguing was that it was reportedly trained using less
compute than its closest counterparts.</p>
<h2 id="architectural-highlights-1"><a class="header" href="#architectural-highlights-1">Architectural Highlights</a></h2>
<p>DeepSeek-V3 is a transformer-based model that swaps out nearly all dense <a href="models/../llms/architecture/feedforward.html">feedforward</a>
for <a href="models/../llms/architecture/moe.html">MoE</a>. The model has a total of 671B parameters
but through its specialized variant of MoE (referred to as DeepSeekMoE), only
37B parameters are activated in both training and inference. Through a series of
long-context extension fine-tuning steps, the maximum context length for this model
was extended to 128K tokens.</p>
<p><strong>DeepSeekMoE:</strong> Used to carry out training more efficiently, this MoE design
consists of two sets of experts, namely: shared and routed. The former set of routers
is used for every token in the input sequence whereas the usage of routed ones are
determined according to the affinity to the input token.</p>
<p><strong>Auxiliary-loss Load Free Balancing:</strong> When using an MoE architecture, one must
consider load balancing across the networks to prevent routing collapse. This has
been typically addressed via the introduction of an auxiliary loss. However, if
this loss has too great of an influence, it can lead to a model degradation. DeepSeek-V3
instead considers a technique that requires no auxiliary loss but instead relies
on a new bias term that dynamically changes its value according to the experts
current workload.</p>
<p><strong>Multi-Head Latent Attention (MLA):</strong> Used for making inference more efficient
by jointly compressing attention keys and values to a lower dimension. The compression
involves a linear projection matrix compressing keys and values down as well as
another linear project matrix for compressing keys and values back up. Only the
compressed joint representation of keys and values need to be cached during inference.
For more details see <a href="models/../llms/architecture/mla.html">MLA</a>.</p>
<p><strong>Multi-Token Prediction:</strong> In an effort to improve the training signal, DeepSeek-V3
expands the prediction scope to additional future tokens at every token position
of the sequence. In other words, instead of only predicting the next immediate token
and training the model on this signal, $D$ future tokens are predicted. These tokens
are predicted sequentially by $D$ sequential multi-token prediction modules in order
to maintain the causal chain. For more details see <a href="models/../llms/decoding/multi_token_prediction.html">MTP</a>.</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody>
<tr><td>Total parameters</td><td>671B</td></tr>
<tr><td>Activated parameters</td><td>37B</td></tr>
<tr><td>Maximum context length</td><td>128K tokens</td></tr>
<tr><td>Number of Transformer layers</td><td>61</td></tr>
<tr><td>Hidden dimension size</td><td>7168</td></tr>
<tr><td>Number of attention heads</td><td>128</td></tr>
<tr><td>Number of experts (MoE)</td><td>1 (shared) &amp; 256 (routed)</td></tr>
<tr><td>Hidden dimension of experts</td><td>2048</td></tr>
<tr><td>KV compression dimension size (MLA)</td><td>512</td></tr>
<tr><td>Multi-token depth (MTP)</td><td>1</td></tr>
</tbody></table>
</div><div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Table 1: Summary of DeepSeek-V3 architecture and hyper parameters.
</div>
<h2 id="training-data-1"><a class="header" href="#training-data-1">Training Data</a></h2>
<p>The pre-training corpus is a revised version of the one used to train an earlier
version of the model, DeepSeek-V2. In this revision, more samples pertaining to
mathematics and programming were included. Ultimately, the dataset comprised of
14.8T tokens.</p>
<h2 id="compute-details"><a class="header" href="#compute-details">Compute Details</a></h2>
<p>DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs. Each node within
the cluster consists of 8 H800 GPUs inter-connected via NVLink and NVSwitch. In total,
it was reported that only 2.664M H800 GPU hours were used for pre-training while
subsequent training stages required only 0.1M GPU hours. One of the main reasons
for this training efficiency was their application of an FP8 mixed precision
training framework.</p>
<h2 id="key-results-1"><a class="header" href="#key-results-1">Key Results</a></h2>
<!-- markdownlint-disable MD013 -->
<div class="table-wrapper"><table><thead><tr><th>Benchmark (Metric)</th><th># Shots</th><th>DeepSeek-V2 Base</th><th>Qwen2.5 72B Base</th><th>LLaMA-3.1 405B Base</th><th>DeepSeek-V3 Base</th></tr></thead><tbody>
<tr><td>Pile-test (BPB)</td><td>-</td><td>0.606</td><td>0.638</td><td><strong>0.542</strong></td><td>0.548</td></tr>
<tr><td>BBH (EM)</td><td>3-shot</td><td>78.8</td><td>79.8</td><td>82.9</td><td><strong>87.5</strong></td></tr>
<tr><td>MMLU (EM)</td><td>5-shot</td><td>78.4</td><td>85.0</td><td>84.4</td><td><strong>87.1</strong></td></tr>
<tr><td>MMLU-Redux (EM)</td><td>5-shot</td><td>75.6</td><td>83.2</td><td>81.3</td><td><strong>86.2</strong></td></tr>
<tr><td>MMLU-Pro (EM)</td><td>5-shot</td><td>51.4</td><td>58.3</td><td>52.8</td><td><strong>64.4</strong></td></tr>
<tr><td>DROP (F1)</td><td>3-shot</td><td>80.4</td><td>80.6</td><td>86.0</td><td><strong>89.0</strong></td></tr>
<tr><td>ARC-Easy (EM)</td><td>25-shot</td><td>97.6</td><td>98.4</td><td>98.4</td><td><strong>98.9</strong></td></tr>
<tr><td>ARC-Challenge (EM)</td><td>25-shot</td><td>92.2</td><td>94.5</td><td><strong>95.3</strong></td><td><strong>95.3</strong></td></tr>
<tr><td>HellaSwag (EM)</td><td>10-shot</td><td>87.1</td><td>84.8</td><td><strong>89.2</strong></td><td>88.9</td></tr>
<tr><td>PIQA (EM)</td><td>0-shot</td><td>83.9</td><td>82.1</td><td><strong>85.9</strong></td><td>84.7</td></tr>
<tr><td>WinoGrande (EM)</td><td>5-shot</td><td><strong>86.3</strong></td><td>82.3</td><td>85.2</td><td>84.9</td></tr>
<tr><td>RACE-Middle (EM)</td><td>3-shot</td><td>73.1</td><td>68.1</td><td><strong>74.2</strong></td><td>74.9</td></tr>
<tr><td>RACE-High (EM)</td><td>5-shot</td><td>52.6</td><td>50.3</td><td><strong>56.8</strong></td><td>51.3</td></tr>
<tr><td>TriviaQA (EM)</td><td>5-shot</td><td>80.0</td><td>71.9</td><td><strong>82.7</strong></td><td>82.9</td></tr>
<tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>38.6</td><td>33.2</td><td><strong>41.5</strong></td><td>40.0</td></tr>
<tr><td>AGIEval (EM)</td><td>0-shot</td><td>57.5</td><td>75.8</td><td>60.6</td><td><strong>79.6</strong></td></tr>
<tr><td>HumanEval (Pass@1)</td><td>0-shot</td><td>43.3</td><td>53.0</td><td>54.9</td><td><strong>65.2</strong></td></tr>
<tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>65.0</td><td>72.6</td><td>68.4</td><td><strong>75.4</strong></td></tr>
<tr><td>LiveCodeBench-Base (Pass@1)</td><td>3-shot</td><td>11.6</td><td>12.9</td><td>15.1</td><td><strong>19.4</strong></td></tr>
<tr><td>CRUXEval-1 (EM)</td><td>2-shot</td><td>52.5</td><td>59.1</td><td>58.5</td><td><strong>67.3</strong></td></tr>
<tr><td>CRUXEval-O (EM)</td><td>2-shot</td><td>49.8</td><td>59.9</td><td>59.9</td><td><strong>69.8</strong></td></tr>
<tr><td>CSMRR (EM)</td><td>8-shot</td><td>81.6</td><td>88.3</td><td>89.3</td><td><strong>89.3</strong></td></tr>
<tr><td>MATH (EM)</td><td>4-shot</td><td>43.4</td><td>54.4</td><td>49.0</td><td><strong>61.6</strong></td></tr>
<tr><td>MGSM (EM)</td><td>8-shot</td><td>63.6</td><td>76.2</td><td>69.9</td><td><strong>79.8</strong></td></tr>
<tr><td>CMath (EM)</td><td>3-shot</td><td>78.7</td><td>84.5</td><td>77.3</td><td><strong>90.7</strong></td></tr>
<tr><td>CLUEWSC (EM)</td><td>5-shot</td><td>82.0</td><td>82.5</td><td><strong>83.0</strong></td><td>82.7</td></tr>
<tr><td>C-Eval (EM)</td><td>0-shot</td><td>81.4</td><td>72.5</td><td>72.5</td><td><strong>90.1</strong></td></tr>
<tr><td>CMMLU (EM)</td><td>5-shot</td><td>84.0</td><td><strong>89.5</strong></td><td>73.7</td><td>88.8</td></tr>
<tr><td>CMRC (EM)</td><td>1-shot</td><td><strong>77.4</strong></td><td>75.8</td><td>76.0</td><td>76.3</td></tr>
<tr><td>C3 (EM)</td><td>0-shot</td><td>77.4</td><td>76.7</td><td><strong>79.7</strong></td><td>78.6</td></tr>
<tr><td>CCPM (EM)</td><td>0-shot</td><td><strong>93.0</strong></td><td>88.5</td><td>78.6</td><td>92.0</td></tr>
<tr><td>MMLU-non-English (EM)</td><td>5-shot</td><td>64.0</td><td>74.8</td><td>73.8</td><td><strong>79.4</strong></td></tr>
</tbody></table>
</div><!-- markdownlint-enable MD013 -->
<div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Table 2: Comparison between DeepSeek-V3 and other representative models.
(Copied from Table 3 of Liu, Aixin, et al (2024).)
</div>
<ol>
<li>
<p><strong>Superior Open-Source Model:</strong> DeepSeek-V3 outperformed all other open-source
models on educational benchmarks (MMLU, MMLU-Pro, GPQA) achieving performance
levels that rivals that for closed-source models such as GPT-4o and Claude-Sonnet-3.5.
DeepSeek-V3 also achieved SOTA on math-related benchmarks (GSM8K, MATH, MGSM,
CMath).</p>
</li>
<li>
<p><strong>Efficient Training:</strong> DeepSeek-V3 was trained using only 2.664M H800 GPU hours,
leveraging an FP8 mixed precision training framework. This marked, as reported
by the authors, the first successful use of an FP8 scheme to train a large-scale
model.</p>
</li>
<li>
<p><strong>Reasoning Distillation:</strong> As part of the post-training step, DeepSeek-V3 creators
were able to distill reasoning capabilities via long <a href="models/../llms/prompting/cot.html">CoT</a>
passages generated by <a href="models/../models/deepseek_r1.html">DeepSeek-R1</a>. The authors noted
that this pipeline improved reasononing performance while still maintaining the
ability to produce desired outputs and efficient response lengths.</p>
</li>
</ol>
<h2 id="limitations-2"><a class="header" href="#limitations-2">Limitations</a></h2>
<p>DeepSeek-V3 requires significant amounts of computation facilities to ensure efficient
inference.</p>
<h4 id="references--useful-links-2"><a class="header" href="#references--useful-links-2">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2412.19437"><em>Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint
arXiv:2412.19437 (2024).</em></a></li>
<li><a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">DeepSeek sparks AI stock selloff; Nvidia posts record market-cap loss</a>
(<em>appearing in reuters.com</em>)</li>
</ol>
<!-- markdownlint-disable-file MD033 -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
